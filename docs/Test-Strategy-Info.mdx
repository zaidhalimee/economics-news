import { Meta } from '@storybook/blocks';
import { Link } from 'react-router-dom';

<Meta title="docs/WS-Automation-strategy" />

# WS - Automation strategy

## Background 

This document aims to provide high-level guidance on approaching test automation for World Service teams. It specifically seeks to answer the following questions:


- Why should we automate? (Why we write tests)
- Where should we automate? (What levels)
- What should we automate? 
- When should we automate?
- How should we automate? (Patterns, Anti-Patterns and Practices)
- Who should automate? (Responsibilities)

While this document focuses primarily on e2e test strategy, it touches on other levels of testing to give a holistic understanding and context for the decisions we make when it comes to testing. It documents some of our existing approaches, while suggesting gradual improvements for the future. It’s a living collaborative document that we should refer to and update continuously. 


> You can also check this [related document](https://onebbc-my.sharepoint.com/:w:/g/personal/meriem_mechri01_bbc_co_uk/EftDa1m66I5FuNGzD2gDEdYBSoBe-g9LDAvJf91_K6JuLw?email=Simon.Frampton%40bbc.co.uk&e=jkNFze&wdOrigin=TEAMS-MAGLEV.p2p_ns.rwc&wdExp=TEAMS-TREATMENT&wdhostclicktime=1722460818407&web=1) that describes our current e2e stack.


## Why should we automate?

There are many reasons why we write automated tests. Some of the most common are:


1. Verify the code is working correctly
2. Prevent future regressions
3. Document code behaviour
4. Provides (code) design guidance
5. Support refactoring

[source: https://madeintandem.com/blog/five-factor-testing/]


> 🔊 discussion point with team: any other reasons? what are for you the most important reasons?
    

It is important to think about why  we write tests because it influences what types of testing we should prioritise. For example, unit tests might be good for improving the system design as the units become more independent and easier to test, but having only unit tests doesn’t necessarily verify the code is working correctly (beyond the single unit) as it won’t necessarily reveal problems that occur in the integration of components. On the other hand, E2E tests can provide better verification of the software, but they’re expensive and would not necessarily improve software design, nor document the code behaviour at unit or component level.

## Where should we automate?
## Defining the test levels

Broadly speaking, we follow the test pyramid. We have a large set of unit tests, a smaller subset of integration tests, and an even smaller subset of UI tests. We should continue to follow this pattern.

![Image](https://blog.getmason.io/content/images/2020/11/Testing-pyramid--6--1.jpg)
To avoid confusion, we will clarify what the test levels mean in our context.

### Unit tests
We use jest to write unit tests
They normally exercise a single component (UI components, hooks, helper methods)
Some examples of our unit tests can be found here.
Some patterns:

- We use data-driven tests sometimes like here to define inputs to tests
- We use snapshot tests to easily test some cases like here
    

### Component tests
We conduct visual regression testing using Chromatic, which alongside our unit tests, comprise our component tests layer.

Integration tests
These test the integration between the different components to ensure a page is rendered properly. 
They typically check that what’s rendered matches SIMORGH_DATA in a flexible generic way, i.e. check that a page renders “most read” topics but not the exact topics rendered, and that each topic has an image and header but not the content of either.
Examples can be found in  src/integration.
Check here for more details about our integration tests.

### UI (E2E) tests
These tests run the actual UI in a browser, and can check that everything renders correctly. For example, an integration test might check that “most read” topics are rendered with the correct HTML tags, but that does not necessarily mean they appear correctly (some CSS value might hide them). With the UI tests, we check that everything shows as expected in the UI.
Examples and instructions can be found in cypress.



> 🔊 discussion point with team: should we break UI / e2e tests as @Simon F comment mentions - I agree with the comment, but I am not sure this reflects what we do in our team right now?


#### What levels to prioritise?

Unit tests responsibility and scope are relatively easy to define: we already have a high level of code coverage and developers should aim to maintain that. 

For integration and e2e tests, the general rule of thumb should be to only add an e2e test when an integration test is not enough to ensure coverage. Integration tests are less expensive and more stable and they should be prioritised.


> ℹ️ Think about the maintenance cost of adding a new e2e test. Always favour an integration test if possible instead of an end to end test.

For new features, we should decide as part of the planning or shaping of work/tickets whether an e2e test is necessary for the feature.

### What should we automate?

The decision to write an e2e test should be taken during planning as a team, but these are general guidelines to what should go into e2e tests

#### Critical paths 
The critical path is the set of components and workflows that is required for the application to serve its core function. For example, an article page displaying the content and title for the given service is critical in our context, but links to related topics or top stories, can be considered non-critical. These should be tested as well, but they’re better candidates for a less expensive layer of testing like integration tests.

#### Smoke tests
We should have smoke tests for validating that core functionality is not broken, for example, that the page loads, has titles and content. These should run as often as possible (on each PR), they should be quick, reliable and repeatable (not flaky).

#### As little as possible
The rule of thumb is that we should avoid writing an end to end test if there is an alternative to validating the functionality. The alternative can be a combination of layers, for example, an integration test combined with an API test (which doesn’t exist right now) can be enough to reach the same confidence as an e2e test with less cost.

### Puma approach
Adopting the PUMA approach can provide a framework to decide if a proposed test genuinely meets the definition of an E2E. According to PUMA, an e2e test should:
P - Prove core functionality, 
U - be Understood by all,
M - Mandatory
A - Automated


### How should we automate?
Patterns, anti-patterns and practices for e2e tests

In this section, we will document some of the patterns we use and the best practices to write better e2e tests.

#### Cypress best practices
A good place to start is the Cypress best practices page: https://docs.cypress.io/guides/references/best-practices

Custom commands for common functionalities
We have many custom commands that make common tasks easier, these are located under cypress/e2e/commands. It’s good to have a look at these to have an overview of what’s possible, and consider abstracting common functionality as a cypress command when it makes sense.

#### Do not ignore “flaky” tests


> There is no such thing as a flaky test.
> 
> Any test is designed to provide insight into the functionality of the piece of code it exercises. As a result, when the test completes it provides information about how the code performed the actions detailed in the test.
> 
> If the test is “green” (i.e. it confirms the actual behaviour matches the expected behaviour), then we are provided with the knowledge that at that time the system behaved as defined by the test.
> 
> If the test is “red” (i.e. it confirms the actual behaviour did not match expectation), then we are provided with the knowledge that at that time the system did not behave as defined by the test.
> 
> If a test remains permanently “red”, then we either misunderstood the purpose of the code and so need to review the expectations of the checks in the test or we have uncovered unexpected behaviour in the system under test.
> 
> If a test remains permanently “green”, then we confirm current behaviour continues to meet the expectations of the test.
> 
> However, a test that flickers between the two states indicates something is not right and should not be ignored! I believe describing these as flaky makes it easy to ignore potential problems either in the functionality of the system under test or the test itself.
> 
> quote from  @Simon F 



> 🔊 Discussion with team: What are other good practices? any anti-patterns?


Who should automate?
Responsibilities
|             | Unit tests         | Integration        | E2E                |
| ----------- | ------------------ | ------------------ | ------------------ |
| Responsible | Developers         | Developers and QA* | QA and Developers* |
| Accountable | Developers         | Developers         | QA                 |
| Consulted** | Developers and QA# | Developers and QA  | Developers and QA  |
| Informed**  | Developers and QA# | Developers and QA  | Developers and QA  |

* For integration tests, developers are accountable for them, but sometimes QA can be involved in their implementation. For e2e, QA are accountable for them but developers can be involved in the implementation too.

** Consulting and informing in this context happens mainly during the sprint planning or three amigos. As a team, we can decide on whether a feature requires an integration or e2e test (or both). Consulting also includes code reviews of tests being added.

##### There is no reason why QA cannot be consulted and informed about the content of unit tests. As specialists, they might consider and uncover edge cases/scenarios that do not occur to developers and can also help to review the language used to frame the tests (think about `describe` and `it` blocks) even if they are not confident reading the test code itself.


##### References
- Five Factor Testing
- Practical test pyramid
- https://confluence.dev.bbc.co.uk/display/podtest/PUMA
